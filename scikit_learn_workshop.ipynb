{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Scikit-Learn Workshop 14th August 2019\n",
    "\n",
    "![title](images/pydata_cardiff.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outline and Main Aims\n",
    "\n",
    "This workshop will provide a basic introduction to using the API in the [scikit-learn](https://scikit-learn.org/stable/) Python machine learning library. The following topics will be demonstrated:\n",
    "\n",
    "* How the API is used to fit a model - and predict outputs from new data points\n",
    "* The difference between regression and classification problems\n",
    "* How to visualise the model outputs\n",
    "* Examples of using metrics to assess model performance\n",
    "* Methods for post-hoc examination of features in the model\n",
    "\n",
    "For the visualisations and post-hoc analysis, examples will be shown using additional machine learning libraries.\n",
    "\n",
    "### Expectation Management\n",
    "\n",
    "Machine learning is a __massive__ topic! This cannot be stressed enough! There is still active research development into various aspects in the field, and this will most likely continue in the coming decades. This workshop is intended to provide an introduction to new users, as well as a quick whirlwind tour into some additional techniques and libraries that you can use alongside scikit-learn.\n",
    "\n",
    "The important thing to remember is that machine-learning, together with other methods (or definitions) of Artificial Intelligence and more traditional statistical learning methods, is a lot more involved that just installing a library and hitting 'GO'. All of these techniques are meant to work alongside background research, and domain expertise in any particular field.\n",
    "\n",
    "Here is a brief list of topics that cannot be convered today:\n",
    "\n",
    "* Detailed description of feature engineering\n",
    "    * Some basic examples will be given - but this is, in itself, a massive area of ongoing study\n",
    "* An in depth explanation of the mathematics behind the algorithms\n",
    "    * Some basic intuition is given, but this material is aimed at a practical level\n",
    "    * It is important to realise the strengths, weaknesses, and assumptions about any technique used\n",
    "    * The [scikit-learn user-guide](https://scikit-learn.org/stable/user_guide.html) documentation pages are a fantastic source of information\n",
    "* Domain specific interpretation of models (very important disclaimer):\n",
    "    * Different fields require different outcomes of models:\n",
    "        * Internet advertising - get the message out to as many as you can afford and use the model to pick the best subset. The _confetti_ approach\n",
    "        * Clinical trials and drug research - you need to be far more certain of any outcome\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Scikit-Learn\n",
    "\n",
    "Points to mention:\n",
    "\n",
    "* Single feature linear regression\n",
    "* Single feature logistic regression\n",
    "* Make blobs linearly separable\n",
    "* Make moons/circles - non-linearly separable\n",
    "    * Find a way to visualise these decision boundaries\n",
    "* Make an overlapping dataset and overfit massively\n",
    "* Show how a CV procedure can ameliorate this\n",
    "* Put this into a pipeline with feature normalisation\n",
    "    * This can be done by transforming the moons first\n",
    "* If time - show a multi class classification\n",
    "* Give an example of a Random Forest:\n",
    "    * Show how the SHAP library works https://github.com/slundberg/shap\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting a wider notebook\n",
    "\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Getting the basic imports - others will be added when introduced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from copy import deepcopy\n",
    "# from mpl_toolkits.mplot3d import Axes3D\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_style('darkgrid')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compulsory _Linear Regression_ example\n",
    "\n",
    "$y = m \\cdot x + c$\n",
    "\n",
    "While this might seem a bit overkill to use machine learning in this circumstance, when more basic statistical approaches will suffice, it is a good first example to show how the models can recover any parameters that we _know_ to be correct - because we have made them!\n",
    "\n",
    "#### Data Creation\n",
    "\n",
    "Here, we will make some artificial data where the data points broadly fit along a line on a 2D graph, made from the following:\n",
    "\n",
    "* The __slope__ of the line _m_\n",
    "* The __intercept__ - the point at which the line crosses the y-axis _c_\n",
    "* Some added Gaussian noise (normally distributed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(22)\n",
    "noise = np.random.randn(200) * 2\n",
    "\n",
    "intercept = -1.7\n",
    "slope = 3.4\n",
    "\n",
    "x_lin_reg = np.linspace(-2, 10, 200)\n",
    "y_lin_reg = intercept + slope * x_lin_reg + noise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data and Targets\n",
    "\n",
    "This has provided us with our input data, and desired outputs. This task is an example of a __supervised__ learning algorithm, whereby the model is given information about the intended outcome of every data point. Its aim is to learn the relationship, so that other data points, that do not have labels, can be assessed after the model is trained.\n",
    "\n",
    "The aim is to __use the x values to predict the y values__ - in other words, we want to recover the intercept and slope, as we can then use that to create any y value from and x value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(16, 8))\n",
    "ax.scatter(x_lin_reg, y_lin_reg);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing the model type\n",
    "\n",
    "Here we use the `LinearRegression` model from the `linear_model` module. scikit-learn relies heavily on the concept of _object oriented programming_. The model that we import is a `class` or `object` that allows a range of functional procedures to be carried out on some data.\n",
    "\n",
    "When we use this model object, we create an __instance__ of the object, and when creating this instance, we can provide some instructions on how it should behave.\n",
    "\n",
    "Each model has specific instructions and functionality, and all of these can be seen in the documentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LinearRegression?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this instance we want the intercept - this can be specified explictly, but it is the default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_model = LinearRegression()\n",
    "\n",
    "# The following is exactly the same!\n",
    "# linear_model = LinearRegression(fit_intercept=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Note that the inputs to the model _must_ be a 2D array\n",
    "\n",
    "Although this might seem strange for a single feature input - it in nevertheless necessary.\n",
    "\n",
    "There are 2 main ways to do this in `numpy`:\n",
    "\n",
    "* Using `reshape` - this reshapes the array as (`n_rows`, `n_columns`) - the -1 is telling `numpy` - 'use as many as you need'\n",
    "* The `newaxis` method using a slicing operation\n",
    "\n",
    "Note that this notebook will use capitals for variables that are in 2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_lin_reg = x_lin_reg.reshape(-1, 1)\n",
    "X_lin_reg = x_lin_reg[:, np.newaxis]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_lin_reg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_model.fit(X_lin_reg, y_lin_reg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Viewing recovered parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_model.intercept_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_model.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Viewing model output\n",
    "\n",
    "Note that the output now contains the signal without the noise - that's ultimately what we want!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_lin_reg = linear_model.predict(X_lin_reg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(16, 8))\n",
    "ax.scatter(x_lin_reg, y_lin_reg)\n",
    "ax.scatter(x_lin_reg, y_pred_lin_reg, c='g');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification problems\n",
    "\n",
    "The previous model was used to fit a _regression_ problem, whereby the target variable contains a large range of values - it is _continuous_ in nature. However, in many cases, we want to predict classification - or membership of a particular group.\n",
    "\n",
    "The first example with be that of _binary_ classification - either 0 or 1. We'll make some random data. If x > 3, then y = 1, otherwise y = 0\n",
    "\n",
    "Note that one ambiguous data point is being added"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(12)\n",
    "\n",
    "x_cat = np.random.uniform(low=-2, high=8, size=200)\n",
    "y_cat = np.where(x_cat > 3, 1, 0)\n",
    "\n",
    "x_cat = np.append(x_cat, 3.5)\n",
    "y_cat = np.append(y_cat, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(16, 8))\n",
    "ax.scatter(x_cat, y_cat);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trying out Linear Regression\n",
    "\n",
    "Remember to add the dimension!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_cat = x_cat.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_model.fit(X_cat, y_cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_cat_pred = linear_model.predict(X_cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(16, 8))\n",
    "ax.scatter(x_cat, y_cat)\n",
    "ax.plot(x_cat, y_cat_pred, c='r');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting a better model\n",
    "\n",
    "The most basic model for use here is called a __Logistic Regression__. But why is it called regression??? This can be very confusing! It is actually because the same mathematic methods are use to fit to a line, but the data is transformed using the function below - this can then be used for classification\n",
    "\n",
    "### Logistic Regression function\n",
    "\n",
    "$\\frac{1}{1 + e^{-x}}$\n",
    "\n",
    "But remember this (as it confused me when I was starting out): __Logistic Regression is used for CLASSIFICATION!!!__\n",
    "\n",
    "We'll now visualise how this function looks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(ar):\n",
    "    return 1 / (1 + np.exp(-ar))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_sig = np.linspace(-5, 8, 200)\n",
    "y_sig = sigmoid(x_sig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(16, 8))\n",
    "ax.plot(x_sig, y_sig, linewidth=3);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How this works for classification\n",
    "\n",
    "We still can map x to y - but now, if y is > 0.5 we classify it as 1, and 0 otherwise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression - with slope and intercept\n",
    "\n",
    "We can still add parameters to this function\n",
    "\n",
    "$\\frac{1}{1 + e^{slope \\cdot x - intercept}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid_extra(ar, slope, intercept):\n",
    "    return 1 / (1 + np.exp(-slope * (ar - intercept)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_sig_extra = np.linspace(-5, 8, 200)\n",
    "y_sig_extra = sigmoid_extra(x_sig_extra, 5, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(16, 8))\n",
    "ax.plot(x_sig_extra, y_sig_extra, linewidth=3);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing and using the model\n",
    "\n",
    "Note that the `solver` is being specified here. This has been obtained by looking at the documentation and seeing that it is best for small problems.\n",
    "\n",
    "As mentioned in the introduction cells, we won't go into the maths behind this here. But importantly: __don't worry__ - just read through the documentation and don't get bewildered with this! The scikit-learn developers have done a fantastic job of making their software usable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_reg = LogisticRegression(solver='liblinear')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_cat = x_cat.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_reg.fit(X_cat, y_cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_cat_pred = log_reg.predict(X_cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sig_fitted = sigmoid_extra(x_sig, log_reg.coef_[0], -log_reg.intercept_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_style('darkgrid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(16, 8))\n",
    "ax.scatter(x_cat, y_cat, c='b');\n",
    "ax.scatter(x_cat, y_cat_pred, c='g')\n",
    "ax.plot(x_sig, sig_fitted, linewidth=3, c='r');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from yellowbrick.classifier import ROCAUC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROCAUC?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualiser = ROCAUC(LogisticRegression(solver='liblinear'), micro=False, macro=False, per_class=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualiser.fit(X_cat, y_cat)\n",
    "visualiser.score(X_cat, y_cat)\n",
    "g = visualiser.poof()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_cat2 = deepcopy(x_cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_cat2[y_cat==1] -= 1.8\n",
    "X_cat2 = x_cat2.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_reg.fit(X_cat2, y_cat)\n",
    "sig_fitted2 = sigmoid_extra(x_sig, log_reg.coef_[0], -log_reg.intercept_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(16, 8))\n",
    "ax.scatter(x_cat2, y_cat)\n",
    "ax.plot(x_sig, sig_fitted2, c='r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualiser = ROCAUC(LogisticRegression(solver='liblinear'), macro=False, micro=False)\n",
    "visualiser.fit(X_cat2, y_cat)\n",
    "visualiser.score(X_cat2, y_cat)\n",
    "g = visualiser.poof()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_blobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = make_blobs(n_samples=300, n_features=2, centers=((1, 1), (5, 5)), cluster_std=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trm = np.array([[1, -2], [-2, 1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X @ trm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_palette('set1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(7, 7))\n",
    "ax.scatter(X[:, 0], X[:, 1], c=y)\n",
    "ax.axis('equal');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_reg2d = LogisticRegression(solver='liblinear')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_reg2d.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = log_reg2d.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(8, 8))\n",
    "ax.scatter(X[:, 0], X[:, 1], c=y_pred)\n",
    "ax.axis('equal');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_prob = log_reg2d.predict_proba(X)[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(8, 8))\n",
    "ax.scatter(X[:, 0], X[:, 1], c=y_prob)\n",
    "ax.axis('equal');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlxtend.plotting import plot_decision_regions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = make_blobs(n_samples=300, n_features=2, centers=((1, 1), (5, 5)), cluster_std=0.6)\n",
    "X = X @ trm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_blobs = SVC(kernel='linear')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_blobs.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_svm_pred = svm_blobs.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(8, 8))\n",
    "ax.scatter(X[:, 0], X[:, 1], c=y_svm_pred)\n",
    "ax.axis('equal');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_circles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = make_circles(n_samples=500, noise=0.05, factor=0.6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(8, 8))\n",
    "ax.scatter(X[:, 0], X[:, 1], c=y)\n",
    "ax.axis('equal');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "circle_df = pd.DataFrame(data=X, columns=['x1', 'x2'])\n",
    "circle_df['squared'] = np.sqrt(circle_df['x1']**2 + circle_df['x2']**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "circle_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_circle = SVC(kernel='linear')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_circle.fit(circle_df, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_linear_pred = svm_circle.predict(circle_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(8, 8))\n",
    "ax.scatter(X[:, 0], X[:, 1], c=y_linear_pred)\n",
    "ax.axis('equal');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_rbf = SVC(gamma='scale')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_rbf.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_rbf_pred = svm_rbf.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(8, 8))\n",
    "ax.scatter(X[:, 0], X[:, 1], c=y_rbf_pred)\n",
    "ax.axis('equal');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_moons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_moons, y_moons = make_moons(n_samples=500, noise=0.2, random_state=27)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(8, 8))\n",
    "ax.scatter(X_moons[:, 0], X_moons[:, 1], c=y_moons);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_moons_1 = SVC(C=1, gamma='scale')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_moons_1.fit(X_moons, y_moons)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(8, 8))\n",
    "plot_decision_regions(X_moons, y_moons, svm_moons_1);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_moons_100 = SVC(C=100, gamma=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_moons_100.fit(X_moons, y_moons)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(8, 8))\n",
    "plot_decision_regions(X_moons, y_moons, svm_moons_100);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(8, 8))\n",
    "plot_decision_regions(X_moons, y_moons, svm_moons_1, zoom_factor=0.1);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1 / (2 * X_moons.var())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "C_values = np.logspace(-1, 3, 5)\n",
    "C_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma_values = np.logspace(-2, 2, 5)\n",
    "gamma_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel_values = ['linear', 'rbf']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {'kernel': kernel_values, 'C': C_values, 'gamma': gamma_values}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agnostic_svc = SVC(random_state=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = GridSearchCV(estimator=agnostic_svc, param_grid=parameters, scoring='roc_auc', cv=4, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.fit(X_moons, y_moons)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_moons_skewed = deepcopy(X_moons)\n",
    "X_moons_skewed[:, 1] *= 1e-6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X1, X2, y1, y2 = train_test_split(X_moons_skewed, y_moons)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_moons_1.fit(X1, y1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = svm_moons_1.predict(X2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(y2, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
