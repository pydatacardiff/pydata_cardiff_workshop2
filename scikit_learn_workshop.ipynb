{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Scikit-Learn Workshop 14th August 2019\n",
    "\n",
    "![title](images/pydata_cardiff.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outline and Main Aims\n",
    "\n",
    "This workshop will provide a basic introduction to using the API in the [scikit-learn](https://scikit-learn.org/stable/) Python machine learning library. The following topics will be demonstrated:\n",
    "\n",
    "* How the API is used to fit a model - and predict outputs from new data points\n",
    "* The difference between regression and classification problems\n",
    "* How to visualise the model outputs\n",
    "* Examples of using metrics to assess model performance\n",
    "* Methods for post-hoc examination of features in the model\n",
    "\n",
    "For the visualisations and post-hoc analysis, examples will be shown using additional machine learning libraries.\n",
    "\n",
    "### Expectation Management\n",
    "\n",
    "Machine learning is a __massive__ topic! This cannot be stressed enough! There is still active research development into various aspects in the field, and this will most likely continue in the coming decades. This workshop is intended to provide an introduction to new users, as well as a quick whirlwind tour into some additional techniques and libraries that you can use alongside scikit-learn.\n",
    "\n",
    "The important thing to remember is that machine-learning, together with other methods (or definitions) of Artificial Intelligence and more traditional statistical learning methods, is a lot more involved that just installing a library and hitting 'GO'. All of these techniques are meant to work alongside background research, and domain expertise in any particular field.\n",
    "\n",
    "Here is a brief list of topics that cannot be convered today:\n",
    "\n",
    "* Detailed description of feature engineering\n",
    "    * Some basic examples will be given - but this is, in itself, a massive area of ongoing study\n",
    "* An in depth explanation of the mathematics behind the algorithms\n",
    "    * Some basic intuition is given, but this material is aimed at a practical level\n",
    "    * It is important to realise the strengths, weaknesses, and assumptions about any technique used\n",
    "    * The [scikit-learn user-guide](https://scikit-learn.org/stable/user_guide.html) documentation pages are a fantastic source of information\n",
    "* Domain specific interpretation of models (very important disclaimer):\n",
    "    * Different fields require different outcomes of models:\n",
    "        * Internet advertising - get the message out to as many as you can afford and use the model to pick the best subset. The _confetti_ approach\n",
    "        * Clinical trials and drug research - you need to be far more certain of any outcome\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine Learning terminology and aims\n",
    "\n",
    "* The main aim of machine learning is to gain information from data that we have, and use it to _infer_ information about new data points that we will receive:\n",
    "* __Supervised Learning__\n",
    "    * We know the target values of the data that we already have\n",
    "        * __Regression__\n",
    "            * Our target values can take a range (usually continuous) of values\n",
    "        * __Classification__\n",
    "            * Our target values can take a finite number of discrete values\n",
    "            * Examples include binary: yes/no, countries in GB: Scotland/Wales/England etc\n",
    "* __Unsupervised Learning__\n",
    "    * We don't have labels in our existing dataset\n",
    "    * We want to find patterns in this data\n",
    "    * Clustering\n",
    "    * This topic was dealt with in the previous workshop\n",
    "    \n",
    "__This workshop will focus on supervised learning, and primarily on classification__\n",
    "\n",
    "* __Samples__:\n",
    "    * Each data point that we have - represented as the ___rows___ in an input matrix or dataframe\n",
    "* __Features__:\n",
    "    * The number of points of information that we have about each datapoint - represented as the ___columns___\n",
    "* __Classes__:\n",
    "    * The number of discrete target values in classification tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting a wider notebook\n",
    "\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Getting the basic imports - others will be added when introduced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from copy import deepcopy\n",
    "import matplotlib\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_style('darkgrid')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compulsory _Linear Regression_ example\n",
    "\n",
    "$y = m \\cdot x + c$\n",
    "\n",
    "While this might seem a bit overkill to use machine learning in this circumstance, when more basic statistical approaches will suffice, it is a good first example to show how the models can recover any parameters that we _know_ to be correct - because we have made them!\n",
    "\n",
    "#### Data Creation\n",
    "\n",
    "Here, we will make some artificial data where the data points broadly fit along a line on a 2D graph, made from the following:\n",
    "\n",
    "* The __slope__ of the line _m_\n",
    "* The __intercept__ - the point at which the line crosses the y-axis _c_\n",
    "* Some added Gaussian noise (normally distributed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(22)\n",
    "noise = np.random.randn(200) * 2\n",
    "\n",
    "intercept = -1.7\n",
    "slope = 3.4\n",
    "\n",
    "x_lin_reg = np.linspace(-2, 10, 200)\n",
    "y_lin_reg = intercept + slope * x_lin_reg + noise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data and Targets\n",
    "\n",
    "This has provided us with our input data, and desired outputs. This task is an example of a __supervised__ learning algorithm, whereby the model is given information about the intended outcome of every data point. Its aim is to learn the relationship, so that other data points, that do not have labels, can be assessed after the model is trained.\n",
    "\n",
    "The aim is to __use the x values to predict the y values__ - in other words, we want to recover the intercept and slope, as we can then use that to create any y value from and x value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(16, 8))\n",
    "ax.scatter(x_lin_reg, y_lin_reg);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing the model type\n",
    "\n",
    "Here we use the `LinearRegression` model from the `linear_model` module. scikit-learn relies heavily on the concept of _object oriented programming_. The model that we import is a `class` or `object` that allows a range of functional procedures to be carried out on some data.\n",
    "\n",
    "When we use this model object, we create an __instance__ of the object, and when creating this instance, we can provide some instructions on how it should behave.\n",
    "\n",
    "Each model has specific instructions and functionality, and all of these can be seen in the documentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LinearRegression?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this instance we want the intercept - this can be specified explictly, but it is the default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_model = LinearRegression()\n",
    "\n",
    "# The following is exactly the same!\n",
    "# linear_model = LinearRegression(fit_intercept=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Note that the inputs to the model _must_ be a 2D array\n",
    "\n",
    "Although this might seem strange for a single feature input - it in nevertheless necessary.\n",
    "\n",
    "There are 2 main ways to do this in `numpy`:\n",
    "\n",
    "* Using `reshape` - this reshapes the array as (`n_rows`, `n_columns`) - the -1 is telling `numpy` - 'use as many as you need'\n",
    "* The `newaxis` method using a slicing operation\n",
    "\n",
    "Note that this notebook will use capitals for variables that are in 2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_lin_reg = x_lin_reg.reshape(-1, 1)\n",
    "X_lin_reg = x_lin_reg[:, np.newaxis]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the rows are the datapoints/samples, and there is only one column as it is a single feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_lin_reg[:10, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The `fit` and `predict` pattern\n",
    "\n",
    "If you're going to remember one thing from this workshop - remember these 2 functions that are called in scikit-learn to build the models, and then use them to make new predictions of unseen data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_model.fit(X_lin_reg, y_lin_reg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Viewing recovered parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_model.intercept_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_model.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Viewing model output\n",
    "\n",
    "Note that the output now contains the signal without the noise - that's ultimately what we want!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_lin_reg = linear_model.predict(X_lin_reg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(16, 8))\n",
    "ax.scatter(x_lin_reg, y_lin_reg)\n",
    "ax.scatter(x_lin_reg, y_pred_lin_reg, c='g');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification problems\n",
    "\n",
    "The previous model was used to fit a _regression_ problem, whereby the target variable contains a large range of values - it is _continuous_ in nature. However, in many cases, we want to predict classification - or membership of a particular group.\n",
    "\n",
    "The first example with be that of _binary_ classification - either 0 or 1. We'll make some random data. If x > 3, then y = 1, otherwise y = 0. But there will be a slight overlap in the data.\n",
    "\n",
    "## Important:\n",
    "\n",
    "The values of classification targets in scikit-learn must be numeric values in an array! So a list of strings with category names is not suitable! This workshop will not deal with feature engineering. That is for another workshop! (Of note - the problem mentioned here can be solved using the `LabelEncoder` function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(12)\n",
    "\n",
    "x_cat = np.random.uniform(low=-2, high=8, size=500)\n",
    "y_cat = np.where(x_cat > 3, 1, 0)\n",
    "x_cat[y_cat == 1] -= 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_cat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(16, 8))\n",
    "ax.scatter(x_cat, y_cat);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trying out Linear Regression\n",
    "\n",
    "Remember to add the dimension!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_cat = x_cat.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_model.fit(X_cat, y_cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_cat_pred = linear_model.predict(X_cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(16, 8))\n",
    "ax.scatter(x_cat, y_cat)\n",
    "ax.scatter(x_cat, y_cat_pred, c='r');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting a better model\n",
    "\n",
    "The most basic model for use here is called a __Logistic Regression__. But why is it called regression??? This can be very confusing! It is actually because the same mathematic methods are use to fit to a line, but the data is transformed using the function below - this can then be used for classification\n",
    "\n",
    "### Logistic Regression function\n",
    "\n",
    "$\\frac{1}{1 + e^{-x}}$\n",
    "\n",
    "But remember this (as it confused me when I was starting out): __Logistic Regression is used for CLASSIFICATION!!!__\n",
    "\n",
    "We'll now visualise how this function looks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(ar):\n",
    "    return 1 / (1 + np.exp(-ar))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_sig = np.linspace(-5, 8, 200)\n",
    "y_sig = sigmoid(x_sig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(16, 8))\n",
    "ax.plot(x_sig, y_sig, linewidth=3);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How this works for classification\n",
    "\n",
    "We still can map x to y - but now, if y is > 0.5 we classify it as 1, and 0 otherwise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing and using the model\n",
    "\n",
    "Note that the `solver` is being specified here. This has been obtained by looking at the documentation and seeing that it is best for small problems.\n",
    "\n",
    "As mentioned in the introduction cells, we won't go into the maths behind this here. But importantly: __don't worry__ - just read through the documentation and don't get bewildered with this!\n",
    "\n",
    "The scikit-learn developers have done a fantastic job of making their software usable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_reg = LogisticRegression(solver='liblinear')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_reg.fit(X_cat, y_cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_cat_pred = log_reg.predict(X_cat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting the results\n",
    "\n",
    "The graph below shows the fitted logistic curves, together with the predictions. As expected, there are some misclassifications where the datapoints overlap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.special import expit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(16, 8))\n",
    "ax.scatter(x_cat, y_cat, c='b');\n",
    "ax.scatter(x_cat, y_cat_pred, c='g')\n",
    "ax.plot(x_sig, expit(x_sig * log_reg.coef_[0] + log_reg.intercept_), c='r');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting probability outputs\n",
    "\n",
    "We can also get an idea of certainty that a model has in our data\n",
    "\n",
    "* __Darker Red__: More certain\n",
    "* __Darker Blue__: Less certain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_cat_proba = log_reg.predict_proba(X_cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(16, 8))\n",
    "ax.scatter(x_cat, y_cat_pred, c=y_cat_proba.max(axis=1), cmap='RdBu_r')\n",
    "ax.plot(x_sig, expit(x_sig * log_reg.coef_[0] + log_reg.intercept_), c='r');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Performance metrics\n",
    "\n",
    "Like many of the other procedures introduced in this workshop - this is a very wide area, and we can only touch on a couple of areas. \n",
    "\n",
    "Methods introduced here:\n",
    "\n",
    "* The confusion matrix:\n",
    "    * This is a quick and simple method to examine correct and incorrect classifications\n",
    "    * The idea situation is to have as many as possible on the top-left to bottom-right diagonal\n",
    "* The 'Receiver-Operator-Characteristic Area-Under-the-Curve' ROC-AUC score\n",
    "    * A more complex score for binary classification problems\n",
    "    * Looks at the trade off between true and false positive as the threshold is changed\n",
    "    * Introduced here as it is often used - and is not immediately intuitive\n",
    "    \n",
    "## Testing on unseen data\n",
    "\n",
    "Whenever assessing the performance of any model - it is important to use datapoints that were not used to train the model. This will be described in greater detail later on, but for now, we will use the `train_test_split` function from the `model_selection` module to split our data, and set aside 10% of the data for getting the metrics only.\n",
    "\n",
    "Also note that this method does a ___stratified___ split. This means that the ratio of classes will remain the same. So, if there is only 20% of positive cases in a dataset, then both the training and test sets will contain 20% of their respective sets as positive classes. In other words - there won't be any chance of either the training or the tests sets taking all the information!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_test_split?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(999)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_cat, y_cat, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_reg.fit(X_train, y_train)\n",
    "y_pred = log_reg.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can see that there are 3 incorrect classifications in the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the visualisation library YellowBrick\n",
    "\n",
    "The [YellowBrick](https://www.scikit-yb.org/en/latest/index.html) library includes an array of visualisation methods to aid with machine learning interpretation\n",
    "\n",
    "At first the confusion matrix is used as before, except the output is clearer and more informative than the plain text output from sklearn.\n",
    "\n",
    "Note that the visualiser is now used to fit and score the model - and note the use of the magical `poof` to get the image. Very _Wizard of Oz!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from yellowbrick.classifier import ConfusionMatrix, ROCAUC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = ConfusionMatrix(LogisticRegression(solver='liblinear'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm.fit(X_train, y_train)\n",
    "cm.score(X_test, y_test)\n",
    "cm.poof()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Resetting some plotting colour settings that got affected by the YellowBrick import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_style('darkgrid')\n",
    "sns.set_palette('tab10')\n",
    "cmap = matplotlib.cm.get_cmap('tab10')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Area Under the Curve (AUC) metric\n",
    "\n",
    "This metric is being covered here, as it is often used in cases of binary classification. One of the reasons why it is so popular is in cases where the classification is 'unbalanced'. This means that you can have many more of one class than another.\n",
    "\n",
    "## Examples in psychiatric medicine\n",
    "\n",
    "A commonly shared statistic of 'lifetime-risk' of schizophrenia is 1%. This means that it is easy to create a 'schizophrenia-predicting' model that is always 99% accurate: just always predict that someone will never suffer from the disease! Simples!\n",
    "\n",
    "However, it is clear that this model is useless, and the use of the AUC metric can help with this.\n",
    "\n",
    "## How it works\n",
    "\n",
    "We've mentioned before that the threshold appears when the y-value reaches 0.5 - but this can be altered. Imagine that we set it much lower, then gradually in the positive direction - from left to right on the sigmoid graph, each time classifying more of the data points as positive (1) values. At the start, we only scoop up the true positives, but as we get to the ambiguous area, we start to pick up some false positives well.\n",
    "\n",
    "In the AUC graph, we map out this progress. The area of this graph is 1 by 1 - given an area also of 1. We start off in the bottom left of the graph. For every correct classification that we scoop up, we draw a bit upwards - but as we get any wrong ones - we move to the right. Eventually - all of them will be classed as positive, so we reach the top right. The aim is to move up as much as possible before we head right. We then take the 'area under this curve'.\n",
    "\n",
    "If the data is completely random - or our model is useless, we will collect true and false positives at random, and the line will head in a diagonal manner. This would give us an area of 0.5. But if the data is all classified perfectly, we will collect all of the correct answers before we start getting any mistakes, meaning that we will hit the top left corner, and the area will be 1.\n",
    "\n",
    "__The AUC metric therefore goes from worst to best with values of 0.5 to 1__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Starting with our existing 'clear' dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rocauc1 = ROCAUC(LogisticRegression(solver='liblinear'), micro=False, macro=False, per_class=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rocauc1.fit(X_train, y_train)\n",
    "rocauc1.score(X_test, y_test)\n",
    "g = rocauc1.poof()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making some more obsure data\n",
    "\n",
    "We'll now tweak the dataset so that it there will a considerable overlap, and it is not as easy to clarify. In the beginning, we'll fit the model to the whole data, so that we can get an idea of the shape of the logistic curve that is fitted.\n",
    "\n",
    "### Note that we are using the same `log_reg` object. We don't need to make a new one each time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_cat2 = deepcopy(x_cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_cat2[y_cat==1] -= 2\n",
    "X_cat2 = x_cat2.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_reg.fit(X_cat2, y_cat)\n",
    "sig_fitted2 = expit(x_sig * log_reg.coef_[0] + log_reg.intercept_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(16, 8))\n",
    "ax.scatter(x_cat2, y_cat)\n",
    "ax.plot(x_sig, sig_fitted2, c='r');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting the data then getting the AUC curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(911)\n",
    "X_train_overlap, X_test_overlap, y_train_overlap, y_test_overlap = train_test_split(X_cat2, y_cat, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rocauc2 = ROCAUC(LogisticRegression(solver='liblinear'), macro=False, micro=False)\n",
    "rocauc2.fit(X_train_overlap, y_train_overlap)\n",
    "rocauc2.score(X_test_overlap, y_test_overlap)\n",
    "g = rocauc2.poof()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Other Scoring metrics\n",
    "\n",
    "There are a number of different scoring metrics that can be used in many different situations. Those available in sklearn can be read about [here](https://scikit-learn.org/stable/modules/classes.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Looking at 2D data - 2 features\n",
    "\n",
    "The following code creates some artificial blob data using the `make_blobs` function in sklearn. By default - these blobs are created as circles, but a transformation matrix is being used to change their shape. Don't worry if you are not familiar with the linear algebra involved here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_blobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(123)\n",
    "X, y = make_blobs(n_samples=300, n_features=2, centers=((1, 1), (5, 5)), cluster_std=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trm = np.array([[1, -2], [-2, 1]])\n",
    "X = X @ trm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(7, 7))\n",
    "ax.scatter(X[:, 0], X[:, 1], c=cmap(y))\n",
    "ax.axis('equal');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_reg2d = LogisticRegression(solver='liblinear')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting the predictions\n",
    "\n",
    "When visualising 2D data - there is a really nice function in the [mlxtend](http://rasbt.github.io/mlxtend/) library called `plot_decision_regions`, which shows the area of coverage of the models.\n",
    "\n",
    "One of the inputs to this function needs to be a fitted model.\n",
    "\n",
    "This library is written by [Sebastian Raschka](https://sebastianraschka.com/), who has written a number of books on Python machine learning that you should definitely check out at some point!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlxtend.plotting import plot_decision_regions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_reg2d.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10, 10))\n",
    "plot_decision_regions(X, y, log_reg2d, ax=ax);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using a Support Vector Machine\n",
    "\n",
    "We will now look at a different algorithm called the Support Vector Machine or SVM. We are using this, as this can now move beyond any problems that are linearly separable.\n",
    "\n",
    "An SVM works as a 'maximum margin classifier' - meaning that the separating line  is chosen so that there is the maximum amount of perpendicular space between the different classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = make_blobs(n_samples=300, n_features=2, centers=((1, 1), (5, 5)), cluster_std=0.6, random_state=9)\n",
    "X = X @ trm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_blobs = SVC(kernel='linear')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_blobs.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10, 10))\n",
    "plot_decision_regions(X, y, svm_blobs, ax=ax);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Non linear problems\n",
    "\n",
    "In the next example - we will look at a problem that cannot be solved with a straight dividing line. We can see that there is a clear circular pattern, but a linear algorithm cannot work this out. This is an example of ___interaction___ within features. The information about the outcome is not depending on a _linear combination_ of the feature _main-effects_ by themselves. The input value of a feature at a certain value is dependent on the value of another feature at any point.\n",
    "\n",
    "One way of solving this is to use __feature engineering__, we can create a feature of the Euclidean distance, and enter that into the model.\n",
    "\n",
    "Another way is to use a __kernel method__ in the SVM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_circles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = make_circles(n_samples=500, noise=0.02, factor=0.6, random_state=987)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(8, 8))\n",
    "ax.scatter(X[:, 0], X[:, 1], c=cmap(y))\n",
    "ax.axis('equal');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This next cell shows that pandas dataframes can be entered into scikit learn as well as numpy arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "circle_df = pd.DataFrame(data=X, columns=['x1', 'x2'])\n",
    "circle_df['squared'] = np.sqrt(circle_df['x1']**2 + circle_df['x2']**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "circle_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We are still using a linear algorithm here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_circle = SVC(kernel='linear')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_circle.fit(circle_df, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_linear_pred = svm_circle.predict(circle_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This shows that the predictions using this new feature are 100% correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(8, 8))\n",
    "ax.scatter(X[:, 0], X[:, 1], c=cmap(y_linear_pred))\n",
    "ax.axis('equal');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using a Radial Basis Function Kernel\n",
    "\n",
    "This shows the real power of using a Support Vector Machine. In short, it allows for non-linear classification. Imagine that a new, third, dimension has been added, and now the centre of the circle is coming out of the screen towards you. Now imagine that the circle boundary is like a contour on a map, and that becomes our new decision boundary.\n",
    "\n",
    "Note that we are specifying one of the arguments when creating the instance, as the latest version of scikit-learn provides a warning otherwise. This will be explained later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_rbf = SVC(kernel='rbf', gamma='auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_rbf.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10, 10))\n",
    "plot_decision_regions(X, y, svm_rbf, ax=ax, zoom_factor=3);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyper-parameters\n",
    "\n",
    "When building machine learning models, there are a number of different settings that change how the algorithm behaves. This really becomes the soul of machine learning - and when people talk of 'training' the models - what is meant is finding the best hyperparameters possible.\n",
    "\n",
    "## Notes on an SVM\n",
    "\n",
    "An SVM can deal with overlapping problems - but in different ways:\n",
    "\n",
    "* It wants to get as much as possible in the training set correct\n",
    "    * But this can pick up signal in any noise present\n",
    "* It allows for more mistakes in the training set\n",
    "    * But can hopefully result in better generalisation to new data\n",
    "    \n",
    "* In an SVM - it is the data points close to the borders that have the influence - these are the __Support Vectors__\n",
    "    * The hyperparameter ___gamma___ specifies the sphere of influence of the support vectors\n",
    "        * The __Larger__ this value - the __less__ of an influence it has - this can result in more specific boundaries, but can pick up on noise\n",
    "            * This is a __High Variance__ model\n",
    "        * In the opposite situation - there is more generalisation - but there is the risk of not picking up relevant signal\n",
    "            * This is a __High Bias__ model\n",
    "    * The hyperparameter ___C___ specifies how much focus the model should put on getting the training set correct\n",
    "    \n",
    "#### Conceptual understanding of the 2 hyperparameters\n",
    "\n",
    "* Think of _gamma_ as deciding how specific the border between classes is\n",
    "* Think of _C_ as how strict this border has to be adhered to"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## An example of general vs over-fitted model\n",
    "\n",
    "The next few cells will show the difference between a model that clearly generalises well, vs a model that is clearly too specific.\n",
    "\n",
    "We are making the overfitted model by specifying high values for both _gamma_ and _C_\n",
    "\n",
    "__Note__: For this example, the focus is on how the decision plane is made, so no splitting into training and test is made here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_moons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_moons, y_moons = make_moons(n_samples=500, noise=0.2, random_state=27)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(8, 8))\n",
    "ax.scatter(X_moons[:, 0], X_moons[:, 1], c=cmap(y_moons));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_moons_1 = SVC(C=1, gamma='scale')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_moons_1.fit(X_moons, y_moons)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(8, 8))\n",
    "plot_decision_regions(X_moons, y_moons, svm_moons_1, ax=ax);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_moons_100 = SVC(C=100, gamma=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_moons_100.fit(X_moons, y_moons)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overfitted decision regions\n",
    "\n",
    "Note how this model has placed far too much importance on the data used to train the model. It has even isolated little islands within each respective area.\n",
    "\n",
    "This model is not suitable for applying to new data points at all."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(8, 8))\n",
    "plot_decision_regions(X_moons, y_moons, svm_moons_100, ax=ax);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A word of caution when interpretting models!\n",
    "\n",
    "Note the following property of the better moon model that we created. If we zoom out enough, we can see that a great deal of the input space is classified as blue/0. However, if we were to get any new data in this region, this might not be the case.\n",
    "\n",
    "In short, we should always be cautious of any classification of new data that is vastly different from the data used to train the model.\n",
    "\n",
    "__Don't just build models and then chuck in data expecting everything to be ok!__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(8, 8))\n",
    "plot_decision_regions(X_moons, y_moons, svm_moons_1, zoom_factor=0.1);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross Validation to find the correct hyper-parameters\n",
    "\n",
    "This really is the whole crux of making machine learning models. If we have the correct model, and the correct hyper-parameter settings for every task, then we would all be millionaires from winning [Kaggle](https://www.kaggle.com/) competitions!\n",
    "\n",
    "This is another example of a __vast__ field of ongoing research. Here we will use a `GridSearch`. This works by carrying out a _brute-force/exhaustive_ search of all possibilities you have stated. The clear disadvantage of this is that it can be __slow__! But it is a clear first example, that is easy to understand. There is active research and development on techniques that look to intelligently search through the hyper-parameter space. One such example is the [Hyperopt-sklearn](http://hyperopt.github.io/hyperopt-sklearn/) library - but that cannot be covered here.\n",
    "\n",
    "### Setting out the search space\n",
    "\n",
    "For this example, we will search through values of _C_ and _gamma_, for both the linear and rbf kernels. (Note that _gamma_ is not actually used in a linear kernel). We are using the `logspace` function from `numpy` to get a wide range of values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "C_values = np.logspace(-1, 3, 5)\n",
    "C_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma_values = np.logspace(-2, 2, 5)\n",
    "gamma_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel_values = ['linear', 'rbf']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Number of combinations\n",
    "\n",
    "These setting alone give 50 different combinations that will all be used!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We pass in the parameter options in a dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {'kernel': kernel_values, 'C': C_values, 'gamma': gamma_values}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting up the model that will be used - the random state is used to aid in reproducibility for the workshop!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agnostic_svc = SVC(random_state=12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A note on the arguments used\n",
    "\n",
    "* `estimator`: A scikit-learn model instance\n",
    "* `param_grid`: The dictionary of parameters that we made\n",
    "* `scoring`: Defines how the model will be judged - refer to the documentation for more information\n",
    "* `cv`: How the data will be split into training and test\n",
    "    * This can get complicated!\n",
    "    * An integer value dictates __K-Fold Stratified Cross Validation__\n",
    "        * The data is split into K-parts - each containing the same ratio of classes\n",
    "        * The data is trained and tested __K__ times, which each part getting to be the test set once\n",
    "        * The aggregated score for the __K__ splits is used to judge that particular set of parameters\n",
    "* `n_jobs`: State if you want parallel processing\n",
    "    * This can help with speeding the process up, if you have multi-core processor (you probably do unless you're still running Windows XP)\n",
    "    * A value of `-1` just means __USE EVERYTHING__\n",
    "    * This can only work on a single machine - it won't magically work on a Hadoop cluster :("
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = GridSearchCV(estimator=agnostic_svc, param_grid=parameters, scoring='roc_auc', cv=4, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "clf.fit(X_moons, y_moons)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recovering the best score - together with the best parameters\n",
    "\n",
    "Note - after doing all of this - you don't need to make a new model with these parameters to predict new data - just use what you have built - in this case the `clf` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a Pipeline\n",
    "\n",
    "This is another important aspect of machine learning, especially if your data is not ready for putting into the model in its raw state.\n",
    "\n",
    "In a Support Vector Machine - we want all of the features in the data to have similar ranges, ie... you want the variance to be similar. Some models are not as sensitive to this, like Random Forests.\n",
    "\n",
    "To show this, we will alter one of the features by making it proportionally much smaller. It can be seen that this has a poor outcome on performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_moons_small = deepcopy(X_moons)\n",
    "X_moons_small[:, 1] *= 1e-6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_moons, y_moons)\n",
    "X_train_small, X_test_small, y_train_small, y_test_small = train_test_split(X_moons_small, y_moons)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roc_moons = ROCAUC(clf, micro=False, macro=False, per_class=False)\n",
    "roc_moons.fit(X_train, y_train)\n",
    "roc_moons.score(X_test, y_test)\n",
    "roc_moons.poof()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roc_moons = ROCAUC(clf, micro=False, macro=False, per_class=False)\n",
    "roc_moons.fit(X_train_small, y_train_small)\n",
    "roc_moons.score(X_test_small, y_test_small)\n",
    "roc_moons.poof()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Solving this - add a normalisation preprocessing step to the model\n",
    "\n",
    "This has the effect of assurring that every feature entered into the training model has the same variance\n",
    "\n",
    "2 main methods:\n",
    "\n",
    "* __Normalisation__:\n",
    "    * Ensuring that the values fit between a range of 0 and 1\n",
    "* __Standardisation__:\n",
    "    * Get the _z-score_ of each data point by using the mean and standard-deviation\n",
    "    * This is the one that will be shown here - using `StandardScaler` function from the `preprocessing` sub-module\n",
    "\n",
    "## Avoid giving the data extra information!\n",
    "\n",
    "A __very__ important thing to remember is that the test data should not receive ___any___ information on what is done with the training\n",
    "\n",
    "When we want to standardise, we should get the mean and standard-deviation from ___the training data only___, and then apply this to the test data.\n",
    "\n",
    "__This is also the case with every K-Fold iteration in the Cross Validation procedures!!__\n",
    "\n",
    "While this might sound like a nightmare to implement, it's not - the sklearn developers have thought of it in advance for us!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting the relevant imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### First we need to outline the steps that we want to carry out using a list of tuples\n",
    "\n",
    "Note that we pass in an _instance_ of the class - either assigned to variables or by creating them in the tuples using `()`\n",
    "\n",
    "The tuples work as follows:\n",
    "\n",
    "* Name of stage - provided as a string\n",
    "* The instance of the object carrying out the stage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler_stage = StandardScaler()\n",
    "svc_stage = SVC()\n",
    "\n",
    "pipeline_steps = [('scaler', scaler_stage), ('svc', svc_stage)]\n",
    "\n",
    "# The following also works\n",
    "# pipeline_steps = [('scaler', StandardScaler()), ('svc', SVC())]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We then use this to create the pipeline - this is now the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_model = Pipeline(pipeline_steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `GridSearch` with a pipeline\n",
    "\n",
    "The general process works the same - we need to ensure that the grid search know which stage of the pipeline the grid values are referring to.\n",
    "\n",
    "Here is where we use the name of the stage passed into the tuples.\n",
    "\n",
    "Note that here, we are only doing a grid search for the SVC hyper-parameters. The scaling does not require any.\n",
    "\n",
    "__Really note the use of the double underscore here!!! Following by the exact name of the parameter__. We will also use the same grid values that we used earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_grid_params = {'svc__C': C_values, 'svc__gamma': gamma_values}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now we use the `GridSearch` is exactly the same way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_grid = GridSearchCV(pipeline_model, pipeline_grid_params, cv=4, n_jobs=-1, scoring='roc_auc')\n",
    "pipeline_grid.fit(X_train_small, y_train_small)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Looking at the best score and parameters\n",
    "\n",
    "Here we can immediately see that the performance during the cross validation procedure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_grid.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_grid.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How does it perform on the test dataset\n",
    "\n",
    "Note that this can now take the unscaled input directly\n",
    "\n",
    "Spoiler - it's brilliant!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_small[:10, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that there is some rounding in the graph label here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_roc = ROCAUC(pipeline_grid, micro=False, macro=False, per_class=False)\n",
    "pipeline_roc.fit(X_train_small, y_train_small)\n",
    "pipeline_roc.score(X_test_small, y_test_small)\n",
    "pipeline_roc.poof()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using ROC without YellowBrick\n",
    "\n",
    "We can get this score from sklearn directly. But we __need__ to provide the score how sure we are of each data point in the test set.\n",
    "\n",
    "Remember for the `LogisticRegression`, we used `predict_proba`. In an SVM, we can use the `decision_function` function instead of `predict`.\n",
    "\n",
    "You will have to look at the documentation to find out what's needed (if possible) for each model you are using!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_small = pipeline_grid.decision_function(X_test_small)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this - we pass in the true values first, then the predicted value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roc_auc_score(y_test_small, y_pred_small)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Larger worked example using a Random Forest - with interpretation of input features\n",
    "\n",
    "For the last example, we will look at using a larger dataset for a slightly more complicated task. For this, we will look at the wine quality dataset that is included in scikit-learn. This consists of the following:\n",
    "\n",
    "* 13 numeric features - all with different variances and ranges\n",
    "* 3 output classes for classification\n",
    "\n",
    "## Random Forests\n",
    "\n",
    "Random Forests are an example of an __Ensemble__ training method. The main idea is that it is a collection of different decision trees, that are all used to tackle a problem. A very brief and simplistic explanation is provided below.\n",
    "\n",
    "A decision tree looks to search through patterns of data in various features, and tries to find aspects of the data that most explain the differences between the classes. It then builds a series of _binary_ splits through the data. You follow the splits, based on different values in the data, until reaching a class label for a particular dataset. However, decision trees can be very prone to overfitting. To get around this, _several_ decision trees are built, but each one is only given a subset of the features. This means that each tree has to do its best, with limited information.\n",
    "\n",
    "When each decision tree is built - their _collective_ vote decides the class of each new sample. They are therefore referred to as __Ensembles of weak classifiers__.\n",
    "\n",
    "Some points to note about Random Forests:\n",
    "\n",
    "* There are many hyper-parameters that can be used and adjusted\n",
    "* They are ___invariant___ to feature scaling:\n",
    "    * This means that, unlike the Support Vector Machines, they are not sensitive to different ranges in the data.\n",
    "    * This is because they look at the information contained in each feature individually - the interaction comes from the collective vote of the decision trees.\n",
    "* They can provide a summary of how much each feature was used when making the final decision\n",
    "    * This can aid in interpretability of the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_wine\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading in the data, and getting information about it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_wine_data = load_wine()\n",
    "print(all_wine_data['DESCR'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wine_input_data = all_wine_data['data']\n",
    "wine_targets = all_wine_data['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wine_input_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wine_targets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting values for some of the hyper-parameters\n",
    "\n",
    "Take special care with the following parameters, as they can cause each decision tree to be very large:\n",
    "\n",
    "* `max_depth`: The number of splits that can be made - if left at default, the splits will continue as much as possible!\n",
    "* `min_samples_split`: The number of samples whose information is needed for a split. The default is 2 and means that splits can be made for every pair in the dataset!\n",
    "\n",
    "Fortunately - this dataset is not all that large"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wine_train, wine_test, target_train, target_test = train_test_split(wine_input_data, wine_targets, test_size=0.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(n_estimators=10, random_state=12, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf.fit(wine_train, target_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = rf.predict(wine_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results\n",
    "\n",
    "So at this point, I was hoping to see some errors that needed to be improved in order to show some cross validation - but the Random Forest totally crushed it!\n",
    "\n",
    "The following image might help with this:\n",
    "\n",
    "![title](images/precision_recall.png)\n",
    "\n",
    "Another very useful piece in the scikit-learn toolbox is the `classification_report`. This shows the following pieces of information:\n",
    "\n",
    "* __Precision__: Suffers from too many false positives\n",
    "* __Recall__: Suffers from too many false negatives\n",
    "* __F1 Score__:\n",
    "\n",
    "![title](images/f1_score.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(target_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Showing that indeed, every prediction is correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Looking at feature importances\n",
    "\n",
    "As mentioned earlier - a feature of Random Forests is that they are able to provide information about the features that they used the most when making their decisions.\n",
    "\n",
    "This can be seen in the following cells, where I have put the results into a Pandas dataframe in order to make the results clearer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf.feature_importances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_dataset = pd.DataFrame(\n",
    "    dict(\n",
    "        names=all_wine_data.feature_names,\n",
    "        importance=rf.feature_importances_\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_dataset.sort_values('importance', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Permutation Importance\n",
    "\n",
    "Another method of looking at the importance of features is to use permutations. This also works on more _black-box_ algorithms like the SVM.\n",
    "\n",
    "In the cases below, the test set is run several times - but with the values for a feature shuffled each time. The main idea being: if the shuffling does not have much of an effect on an outcome - then it probably was not important - __BUT__ if it does affect the scoring, then we can assume that it is important, and is carrying vital information to the outcome of the prediction of each datapoint."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can use the [eli5](https://eli5.readthedocs.io/en/latest/index.html) (explain it like I'm 5) library to carry this out - note that we can use the different scoring methods that we have spoken about!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import eli5\n",
    "from eli5.sklearn import PermutationImportance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perm = PermutationImportance(rf).fit(wine_test, target_test, scoring='f1')\n",
    "eli5.show_weights(perm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perm = PermutationImportance(rf).fit(wine_test, target_test, scoring='precision')\n",
    "eli5.show_weights(perm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perm = PermutationImportance(rf).fit(wine_test, target_test, scoring='recall')\n",
    "eli5.show_weights(perm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perm = PermutationImportance(rf).fit(wine_test, target_test, scoring='accuracy')\n",
    "eli5.show_weights(perm)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
